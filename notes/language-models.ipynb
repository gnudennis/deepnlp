{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ecad222",
   "metadata": {},
   "source": [
    "# 语言模型\n",
    "\n",
    "*语言模型*（language model）的目标是估计文本序列的联合概率，\n",
    "是自然语言处理的关键。\n",
    "\n",
    "长度为$T$的文本序列的联合概率\n",
    "$P(x_1, x_2, \\ldots, x_T)$，\n",
    "其中 $x_t$（$1 \\leq t \\leq T$）文本序列在时间步$t$的词元（单词或字符）。\n",
    "\n",
    "文本序列的联合概率，\n",
    "可以转化为条件概率\n",
    "$P(x_1, x_2, \\ldots, x_T) = \\prod_{t=1}^T P(x_t  \\mid  x_1, \\ldots, x_{t-1})$。\n",
    "其中单词的概率以及给定前面几个单词后出现某个单词的条件概率，被称为*语言模型的参数*。\n",
    "\n",
    "\n",
    "例如：\n",
    "$\n",
    "P(\\text{deep}, \\text{learning}, \\text{is}, \\text{fun}) =  P(\\text{deep}) P(\\text{learning}  \\mid  \\text{deep}) P(\\text{is}  \\mid  \\text{deep}, \\text{learning}) P(\\text{fun}  \\mid  \\text{deep}, \\text{learning}, \\text{is})\n",
    "$\n",
    "\n",
    "\n",
    "$\n",
    "\\\\\n",
    "$\n",
    "语言模型可以用于语音识别消歧，\n",
    "比如“recognize speech”和“wreck a nice beach”读音相似，\n",
    "很容易通过语言模型来解决。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad15de09",
   "metadata": {},
   "source": [
    "## $n$-gram\n",
    "\n",
    "深度学习的解决方案之前，\n",
    "$n$-gram是最实用的语言模型。\n",
    "假设序列满足$n$阶马尔可夫性质，\n",
    "分别对应$n$元语法。\n",
    "\n",
    "\n",
    "以下分别是“一元语法”（unigram）、“二元语法”（bigram）和“三元语法”（trigram）模型。\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(x_1, x_2, \\ldots, x_t) &=  P(x_1) P(x_2) \\ldots P(x_t),\\\\\n",
    "P(x_1, x_2, \\ldots, x_t) &=  P(x_1) P(x_2  \\mid  x_1) \\ldots P(x_t  \\mid  x_t-1),\\\\\n",
    "P(x_1, x_2, \\ldots, x_t) &=  P(x_1) P(x_2  \\mid  x_1) P(x_3  \\mid  x_1, x_2) \\ldots P(x_t  \\mid  x_t-2, x_t-1).\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ad6e8b",
   "metadata": {},
   "source": [
    "### $n$-gram主要问题\n",
    "\n",
    "- $n$-gram 词频受*齐普夫定律*支配会迅速衰减。解决方案是*拉普拉斯平滑*。\n",
    "- $n$-gram 没有考虑词的语义相似性，比如：cat和feline（猫科动物）可能出现在相关的上下文中。解决方案是*深度学习解决方案*。\n",
    "\n",
    "\n",
    "### 拉普拉斯平滑\n",
    "\n",
    "拉普拉斯平滑法可以有效地处理结构丰富而频率不足的低频词词组。\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\hat{P}(x) & = \\frac{n(x) + \\epsilon_1/m}{n + \\epsilon_1}, \\\\\n",
    "    \\hat{P}(x' \\mid x) & = \\frac{n(x, x') + \\epsilon_2 \\hat{P}(x')}{n(x) + \\epsilon_2}, \\\\\n",
    "    \\hat{P}(x'' \\mid x,x') & = \\frac{n(x, x',x'') + \\epsilon_3 \\hat{P}(x'')}{n(x, x') + \\epsilon_3}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "其中$n$表示训练集中的单词总数，用$m$表示唯一单词的数量。\n",
    "$\\epsilon_1,\\epsilon_2$和$\\epsilon_3$是超参数。\n",
    "以$\\epsilon_1$为例：当$\\epsilon_1 = 0$时，不应用平滑；\n",
    "当$\\epsilon_1$接近正无穷大时，$\\hat{P}(x)$接近均匀概率分布$1/m$。\n",
    "\n",
    "\n",
    "### 齐普夫定律\n",
    "\n",
    "*齐普夫定律*（Zipf's law）是指词频以一种明确的方式迅速衰减。\n",
    "<!-- ![齐普夫定律](img/zipf's_law.png) -->\n",
    "<!-- <img src=\"img/zipf's_law.png\" width=\"350\" height=\"350\"  align=left/> -->\n",
    "<img src=\"img/zipf's_law.png\" width=\"40%\" height=\"40%\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422a7248",
   "metadata": {},
   "source": [
    "消除前几个例外单词后，剩余的所有单词大致遵循双对数坐标图上的一条直线。\n",
    "即第$i$个最常用单词的频率$\\log n_i = -\\alpha \\log i + c$其中$\\alpha$是刻画分布的指数，$c$是常数。\n",
    "\n",
    "齐普夫定律揭示通过计数统计和平滑来建模单词是*不可行*的，\n",
    "因为这样建模的结果会大大*高估尾部单词的频率*。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cada0d9",
   "metadata": {},
   "source": [
    "## 循环神经网络\n",
    "\n",
    "$n$元语法模型单词$x_t$在时间步$t$的条件概率仅取决于前面$n-1$个单词。\n",
    "对于时间步$t-(n-1)$之前的单词，\n",
    "如果我们想将其可能产生的影响合并到$x_t$上，\n",
    "需要增加$n$，然而模型参数的数量也会随之呈指数增长，\n",
    "因为词表$\\mathcal{V}$需要存储$|\\mathcal{V}|^n$个数字。\n",
    "\n",
    "使用隐变量模型 $P(x_t \\mid x_{t-1}, \\ldots, x_1) \\approx P(x_t \\mid h_{t-1})$\n",
    "其中$h_{t-1}$是*隐状态*（hidden state），\n",
    "它存储了到时间步$t-1$的序列信息。\n",
    "\n",
    "时间步$t$处的隐状态由当前输入$x_{t}$和先前隐状态$h_{t-1}$共同决定$h_t = f(x_{t}, h_{t-1})$。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e2a994",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-02T06:34:59.061730Z",
     "start_time": "2022-05-02T06:34:59.010645Z"
    }
   },
   "source": [
    "### 有隐状态的循环神经网络\n",
    "\n",
    "对隐状态使用循环计算的神经网络称为*循环神经网络*（recurrent neural network, RNN），\n",
    "循环神经网络的隐状态可以捕获直到当前时间步序列的历史信息。\n",
    "\n",
    "循环神经网络由*循环层*（recurrent layer）和输出层两部分构成，\n",
    "*循环*是指计算循环，模型参数是固定的，不会随着时间步的增加而增加。\n",
    "\n",
    "<img src=\"img/rnn.svg\" width=\"50%\" height=\"50%\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20df9ab",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{H}_t &= \\phi(\\mathbf{X}_t \\mathbf{W}_{xh} + \\mathbf{H}_{t-1} \\mathbf{W}_{hh}  + \\mathbf{b}_h)\\\\\n",
    "\\mathbf{O}_t &= \\mathbf{H}_t \\mathbf{W}_{hq} + \\mathbf{b}_q\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "其中\n",
    "$\\mathbf{X}_t \\in \\mathbb{R}^{n \\times d}$，\n",
    "$\\mathbf{H}_{t-1} \\in \\mathbb{R}^{n \\times h}$，\n",
    "$\\mathbf{W}_{xh} \\in \\mathbb{R}^{d \\times h}$，\n",
    "$\\mathbf{W}_{hh} \\in \\mathbb{R}^{h \\times h}$，\n",
    "$\\mathbf{b}_h \\in \\mathbb{R}^{1 \\times h}$，\n",
    "$\\mathbf{W}_{hq} \\in \\mathbb{R}^{h \\times q}$，\n",
    "$\\mathbf{b}_q \\in \\mathbb{R}^{1 \\times q}$\n",
    "$\\longrightarrow$\n",
    "$\\mathbf{H}_t \\in \\mathbb{R}^{n \\times h}$，\n",
    "$\\mathbf{O}_t \\in \\mathbb{R}^{n \\times q}$。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0db23dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-02T06:35:32.772278Z",
     "start_time": "2022-05-02T06:35:32.765952Z"
    }
   },
   "source": [
    "### 基于循环神经网络的字符级语言模型\n",
    "\n",
    "语言模型的目标是根据过去的和当前的词元预测下一个词元，\n",
    "因此我们将原始序列移位一个词元作为标签。\n",
    "Bengio等人首先提出使用神经网络进行语言建模\n",
    " :cite:`Bengio.Ducharme.Vincent.ea.2003`。\n",
    " \n",
    "下图是使用循环神经网络构建字符级语言模型。\n",
    "\n",
    "<img src=\"img/rnn-train.svg\" width=\"40%\" height=\"40%\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e6f62c",
   "metadata": {},
   "source": [
    "## 困惑度（Perplexity）\n",
    "\n",
    "可以通过一个序列中所有的$n$个词元的交叉熵损失的平均值来衡量：\n",
    "\n",
    "$$\\frac{1}{n} \\sum_{t=1}^n -\\log P(x_t \\mid x_{t-1}, \\ldots, x_1),$$\n",
    "\n",
    "其中$P$由语言模型给出，\n",
    "$x_t$是在时间步$t$从该序列中观察到的实际词元。\n",
    "这使得不同长度的文档的性能具有了可比性。\n",
    "\n",
    "\n",
    "由于历史原因，自然语言处理的科学家更喜欢使用*困惑度*（perplexity）。\n",
    "\n",
    "$$\\exp\\left(-\\frac{1}{n} \\sum_{t=1}^n \\log P(x_t \\mid x_{t-1}, \\ldots, x_1)\\right).$$\n",
    "\n",
    "困惑度的最好的理解是“下一个词元的实际选择数的调和平均数”。\n",
    "\n",
    "* 在最好的情况下，模型总是完美地估计标签词元的概率为1。\n",
    "  在这种情况下，模型的困惑度为1。\n",
    "* 在最坏的情况下，模型总是预测标签词元的概率为0。\n",
    "  在这种情况下，困惑度是正无穷大。\n",
    "* 在基线上，该模型的预测是词表的所有可用词元上的均匀分布。\n",
    "  在这种情况下，困惑度等于词表中唯一词元的数量。\n",
    "  这是任何实际模型都必须超越这个上限。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca78c880",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "296.398px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
