{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6536e5db",
   "metadata": {},
   "source": [
    "# 手把手搭建 seq2seq（中）：基于注意力机制的实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cc0fb1",
   "metadata": {},
   "source": [
    "## 注意力机制\n",
    "\n",
    "<img src=\"assets/attention-output.svg\" width=\"50%\" height=\"50%\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540ba712",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-11T01:33:20.753673Z",
     "start_time": "2022-05-11T01:33:20.734352Z"
    }
   },
   "source": [
    "* *注意力机制*本质上是一种*汇聚*（attention pooling）操作，相对于全连接层或汇聚层增加了query查询过程。\n",
    "* 注意力汇聚操作可以表示成值的加权和。选择不同的注意力评分函数会带来不同的注意力汇聚操作。\n",
    "* *注意力评分函数*（attention scoring function）经过softmax运算得到*注意力权重*（attention weights）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0c8c6c",
   "metadata": {},
   "source": [
    "### 注意力机制的数学描述"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9759971",
   "metadata": {},
   "source": [
    "假设有一个查询\n",
    "$\\mathbf{q} \\in \\mathbb{R}^q$和\n",
    "$m$个“键－值”对\n",
    "$(\\mathbf{k}_1, \\mathbf{v}_1), \\ldots, (\\mathbf{k}_m, \\mathbf{v}_m)$，\n",
    "其中$\\mathbf{k}_i \\in \\mathbb{R}^k$，$\\mathbf{v}_i \\in \\mathbb{R}^v$。\n",
    "**注意力汇聚函数$f$**就被表示成值的加权和：\n",
    "\n",
    "$$f(\\mathbf{q}, (\\mathbf{k}_1, \\mathbf{v}_1), \\ldots, (\\mathbf{k}_m, \\mathbf{v}_m)) = \\sum_{i=1}^m \\alpha(\\mathbf{q}, \\mathbf{k}_i) \\mathbf{v}_i \\in \\mathbb{R}^v,$$\n",
    "\n",
    "\n",
    "其中查询$\\mathbf{q}$和键$\\mathbf{k}_i$的注意力权重（标量）\n",
    "是通过**注意力评分函数$a$** 将两个向量映射成标量，\n",
    "再经过softmax运算得到的：\n",
    "\n",
    "$$\\alpha(\\mathbf{q}, \\mathbf{k}_i) = \\mathrm{softmax}(a(\\mathbf{q}, \\mathbf{k}_i)) = \\frac{\\exp(a(\\mathbf{q}, \\mathbf{k}_i))}{\\sum_{j=1}^m \\exp(a(\\mathbf{q}, \\mathbf{k}_j))} \\in \\mathbb{R}.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e74830",
   "metadata": {},
   "source": [
    "## 注意力模型实现\n",
    "\n",
    "\n",
    "选择不同的注意力评分函数$a$会导致不同的注意力汇聚操作。下面介绍两种流行的注意力模型实现，即Additive Attention和Scaled dot-product Attention，并基于基础注意力模型实现Multi-Head Attention。\n",
    "\n",
    "\n",
    "### Additive Attention\n",
    "\n",
    "Additive Attention的评分函数\n",
    "$$a(\\mathbf q, \\mathbf k) = \\mathbf w_v^\\top \\text{tanh}(\\mathbf W_q\\mathbf q + \\mathbf W_k \\mathbf k) \\in \\mathbb{R}.$$\n",
    "\n",
    "其中可学习的参数是$\\mathbf W_q\\in\\mathbb R^{h\\times q}$、\n",
    "$\\mathbf W_k\\in\\mathbb R^{h\\times k}$和\n",
    "$\\mathbf w_v\\in\\mathbb R^{h}$。\n",
    "\n",
    "\n",
    "* 可以看作将查询和键连结起来后输入到一个多层感知机（MLP）中，包含一个使用$\\tanh$作为激活函的隐藏层。\n",
    "* 适用于查询和键长度不同时的情形。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdb7484",
   "metadata": {},
   "source": [
    "### Scaled dot-product Attention\n",
    "\n",
    "Scaled dot-product Attention的评分函数\n",
    "$$a(\\mathbf q, \\mathbf k) = \\mathbf{q}^\\top \\mathbf{k}  /\\sqrt{d}.$$\n",
    "\n",
    "不需要可学习的参数。\n",
    "\n",
    "在实践中，通常从小批量的角度来考虑提高效率，\n",
    "例如基于$n$个查询和$m$个键－值对计算注意力，\n",
    "其中查询和键的长度为$d$，值的长度为$v$。\n",
    "查询$\\mathbf Q\\in\\mathbb R^{n\\times d}$、\n",
    "键$\\mathbf K\\in\\mathbb R^{m\\times d}$和\n",
    "值$\\mathbf V\\in\\mathbb R^{m\\times v}$的缩放点积注意力是：\n",
    "\n",
    "$$ \\mathrm{softmax}\\left(\\frac{\\mathbf Q \\mathbf K^\\top }{\\sqrt{d}}\\right) \\mathbf V \\in \\mathbb{R}^{n\\times v}.$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* 适用于查询和键是长度相同时的情形，使用Scaled dot-product Attention 计算效率更高。\n",
    "* 假设查询和键的所有元素都是独立的随机变量，\n",
    "  并且都满足零均值和单位方差，\n",
    "  那么两个向量的点积的均值为$0$，方差为$d$。\n",
    "  为确保无论向量长度如何，\n",
    "  点积的方差在不考虑向量长度的情况下仍然是$1$，\n",
    "  将点积除以$\\sqrt{d}$，"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2bd476",
   "metadata": {},
   "source": [
    "### Multi-Head Attention\n",
    "\n",
    "在实践中，当给定相同的查询、键和值的集合时，希望模型捕获序列不同范围的依赖关系（比如短距离依赖和长距离依赖关系）。\n",
    "\n",
    "*多头注意力*（multihead attention）融合了来自于多个注意力汇聚的**不同知识**，这些知识的不同来源于**相同的查询、键和值的不同的子空间**表示。\n",
    "\n",
    "\n",
    "<img src=\"assets/multi-head-attention.svg\" width=\"35%\" height=\"35%\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9d4f18",
   "metadata": {},
   "source": [
    "#### 多头注意力的数学描述\n",
    "\n",
    "给定查询$\\mathbf{q} \\in \\mathbb{R}^{d_q}$、\n",
    "键$\\mathbf{k} \\in \\mathbb{R}^{d_k}$和\n",
    "值$\\mathbf{v} \\in \\mathbb{R}^{d_v}$，\n",
    "每个注意力头$\\mathbf{h}_i$（$i = 1, \\ldots, h$）的计算方法为：\n",
    "\n",
    "$$\\mathbf{h}_i = f(\\mathbf W_i^{(q)}\\mathbf q, \\mathbf W_i^{(k)}\\mathbf k,\\mathbf W_i^{(v)}\\mathbf v) \\in \\mathbb R^{p_v},$$\n",
    "\n",
    "其中可学习的参数包括\n",
    "$\\mathbf W_i^{(q)}\\in\\mathbb R^{p_q\\times d_q}$、\n",
    "$\\mathbf W_i^{(k)}\\in\\mathbb R^{p_k\\times d_k}$和\n",
    "$\\mathbf W_i^{(v)}\\in\\mathbb R^{p_v\\times d_v}$，\n",
    "以及代表注意力汇聚的函数$f$。\n",
    "\n",
    "$f$可以直接使用Additive Attention和Scaled dot-product Attention实现。\n",
    "\n",
    "\n",
    "多头注意力的输出需要经过另一个线性转换，\n",
    "它对应着$h$个头连结后的结果，因此其可学习参数是\n",
    "$\\mathbf W_o\\in\\mathbb R^{p_o\\times h p_v}$：\n",
    "\n",
    "$$\\mathbf W_o \\begin{bmatrix}\\mathbf h_1\\\\\\vdots\\\\\\mathbf h_h\\end{bmatrix} \\in \\mathbb{R}^{p_o}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ec6524",
   "metadata": {},
   "source": [
    "### 代码实现\n",
    "\n",
    "\n",
    "[代码实现详见](https://github.com/gnudennis/deepnlp/blob/master/core/translation/models/attention.py)\n",
    "\n",
    "\n",
    "实现细节：\n",
    "\n",
    "* 计算注意力权重时采用掩码注意力。\n",
    "* 注意力权重与值加权和通常使用dropout正则化。 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe652ef",
   "metadata": {},
   "source": [
    "## 基于注意力模型的实现\n",
    "\n",
    "基于循环神经网络的实现的基础上，增加注意力机制。采用Bahdanau（cite:`Bahdanau.Cho.Bengio.2014`）的方案。\n",
    "\n",
    "<img src=\"assets/seq2seq-attention-details.svg\" width=\"40%\" height=\"40%\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53e4aa5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T08:08:21.591925Z",
     "start_time": "2022-05-08T08:08:21.550715Z"
    }
   },
   "source": [
    "#### 实现细节\n",
    "\n",
    "1. Encoder可以采用双向神经网络实现。\n",
    "2. Bahdanau注意力将上一时间步的解码器隐状态视为查询，在所有时间步的编码器隐状态同时视为键和值。\n",
    "3. 注意力机制的输出同时作为Decoder隐藏层和输出层的输入。\n",
    "\n",
    "\n",
    "<img src=\"assets/seq2seq-attn.png\" width=\"40%\" height=\"40%\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c58a3e",
   "metadata": {},
   "source": [
    "## references\n",
    "\n",
    "* [d2l](https://zh-v2.d2l.ai/chapter_attention-mechanisms/attention-scoring-functions.html)\n",
    "* [pytorch-seq2seq](https://github.com/bentrevett/pytorch-seq2seq)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
