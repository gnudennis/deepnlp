{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6536e5db",
   "metadata": {},
   "source": [
    "# 手把手搭建 seq2seq（下）：Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cc0fb1",
   "metadata": {},
   "source": [
    "## 序列表示的第三种架构：Self-Attention\n",
    "\n",
    "\n",
    "深度学习中，除了CNN和RNN可以对序列进行表示（编码）外，\n",
    "*自注意力*（self-attention `Lin.Feng.Santos.ea.2017, Vaswani.Shazeer.Parmar.ea.2017`）\n",
    "也可以对序列进行表示。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540ba712",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-11T01:33:20.753673Z",
     "start_time": "2022-05-11T01:33:20.734352Z"
    }
   },
   "source": [
    "### 自注意力架构\n",
    "\n",
    "给定一个由词元组成的输入序列$\\mathbf{x}_1, \\ldots, \\mathbf{x}_n$，\n",
    "其中任意$\\mathbf{x}_i \\in \\mathbb{R}^d$（$1 \\leq i \\leq n$）。\n",
    "该序列的自注意力输出为一个长度相同的序列\n",
    "$\\mathbf{y}_1, \\ldots, \\mathbf{y}_n$，其中：\n",
    "\n",
    "$$\\mathbf{y}_i = f(\\mathbf{x}_i, (\\mathbf{x}_1, \\mathbf{x}_1), \\ldots, (\\mathbf{x}_n, \\mathbf{x}_n)) \\in \\mathbb{R}^d$$\n",
    "\n",
    "\n",
    "\n",
    "* $f$可以是任何注意力汇聚函数（Additive Attention、Scaled dot-product Attention、Multi-Head Attention）。\n",
    "* 在自注意力中，QKV来自同一组输入。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0c8c6c",
   "metadata": {},
   "source": [
    "### RNN/CNN/Self-Attention架构比较\n",
    "\n",
    "比较下面几个架构，目标都是将由$n$个词元组成的序列映射到另一个长度相等的序列，其中的每个输入词元或输出词元都由$d$维向量表示。\n",
    "\n",
    "<img src=\"assets/cnn-rnn-self-attention.svg\" width=\"40%\" height=\"40%\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2602aa",
   "metadata": {},
   "source": [
    "假设输入/输出序列长度是$n$，\n",
    "\n",
    "* CNN. 输入/输出通道数均为$d$，卷积核大小为$k$. \n",
    "* RNN. 输入/输出维度均为$d$.\n",
    "* Self-Attention. query/key/value均为$n \\times d$矩阵，假定采用dot-product attention.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "| 架构            | 计算方式 | 计算复杂度             | 最大路径长度 |\n",
    "| :-----         | :----:  | :----: | :----: |\n",
    "| RNN            | 串行     | $\\mathcal{O}(nd^2)$ | $\\mathcal{O}(n)$ |\n",
    "| CNN            | 并行     | $\\mathcal{O}(knd^2)$ | $\\mathcal{O}(n/k)$ |\n",
    "| Self-Attention | 并行     | $\\mathcal{O}(n^2d)$ | $\\mathcal{O}(1)$ |\n",
    "\n",
    "\n",
    "\n",
    "* 顺序操作会妨碍并行计算，而任意的序列位置组合之间的路径越短，则能更轻松地学习序列中的远距离依赖关系。\n",
    "\n",
    "* 卷积神经网络和自注意力都拥有并行计算的优势。自注意力的最大路径长度最短，但其计算复杂度是关于序列长度的二次方，所以在很长的序列中计算会非常慢。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9759971",
   "metadata": {},
   "source": [
    "### 位置编码\n",
    "\n",
    "* 自注意力因为并行计算而放弃了顺序操作。为了使用序列的顺序信息，可以通过在输入表示中添加*位置编码*（positional encoding），来注入绝对的或相对的位置信息。\n",
    "* 位置编码可以通过学习得到也可以直接固定得到，差异不大。\n",
    "\n",
    "下面介绍基于正弦函数和余弦函数的固定位置编码（:cite:`Vaswani.Shazeer.Parmar.ea.2017`）。\n",
    "\n",
    "假设输入表示$\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$\n",
    "包含一个序列中$n$个词元的$d$维嵌入表示。\n",
    "位置编码使用相同形状的位置嵌入矩阵\n",
    "$\\mathbf{P} \\in \\mathbb{R}^{n \\times d}$输出$\\mathbf{X} + \\mathbf{P}$，\n",
    "矩阵第$i$行、第$2j$列和$2j+1$列上的元素为：\n",
    "\n",
    "$$\\begin{aligned} p_{i, 2j} &= \\sin\\left(\\frac{i}{10000^{2j/d}}\\right),\\\\p_{i, 2j+1} &= \\cos\\left(\\frac{i}{10000^{2j/d}}\\right).\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e74830",
   "metadata": {},
   "source": [
    "在位置嵌入矩阵$\\mathbf{P}$中，**行代表词元在序列中的位置，列代表位置编码的不同维度**。\n",
    "基于三角函数的固定位置编码设计可以捕获绝对位置信息和相对位置信息。\n",
    "\n",
    "#### 绝对位置信息\n",
    "\n",
    "先看绝对位置$0, 1, \\ldots, 7$的二进制编码。沿着编码维度\n",
    "```\n",
    "000\n",
    "001\n",
    "010\n",
    "011\n",
    "100\n",
    "101\n",
    "110\n",
    "111\n",
    "```\n",
    "可以看出**沿着编码维度频率递减**，\n",
    "基于三角函数的固定位置编码也有同样的性质。\n",
    "\n",
    "<img src=\"assets/positional_encoding_heatmaps.png\" width=\"50%\" height=\"50%\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdb7484",
   "metadata": {},
   "source": [
    "#### 相对位置信息\n",
    "\n",
    "除了捕获绝对位置信息之外，上述的位置编码还允许模型学习得到输入序列中相对位置信息。\n",
    "\n",
    "对于任何确定的位置偏移$\\delta$，位置$i + \\delta$处的位置编码可以线性投影位置$i$处的位置编码来表示。\n",
    "令$\\omega_j = 1/10000^{2j/d}$，对于任何确定的位置偏移$\\delta$。\n",
    "\n",
    "$$\\begin{aligned}\n",
    "&\\begin{bmatrix} p_{i+\\delta, 2j} \\\\  p_{i+\\delta, 2j+1} \\\\ \\end{bmatrix} \\\\\n",
    "=&\\begin{bmatrix} \\sin\\left((i+\\delta) \\omega_j\\right) \\\\  \\cos\\left((i+\\delta) \\omega_j\\right) \\\\ \\end{bmatrix}\\\\\n",
    "=&\\begin{bmatrix} \\cos(\\delta \\omega_j) \\sin(i \\omega_j) + \\sin(\\delta \\omega_j) \\cos(i \\omega_j) \\\\  -\\sin(\\delta \\omega_j) \\sin(i \\omega_j) + \\cos(\\delta \\omega_j) \\cos(i \\omega_j) \\\\ \\end{bmatrix}\\\\\n",
    "&\\begin{bmatrix} \\cos(\\delta \\omega_j) & \\sin(\\delta \\omega_j) \\\\  -\\sin(\\delta \\omega_j) & \\cos(\\delta \\omega_j) \\\\ \\end{bmatrix}\n",
    "\\begin{bmatrix} p_{i, 2j} \\\\  p_{i, 2j+1} \\\\ \\end{bmatrix}\\\\\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e2bd476",
   "metadata": {},
   "source": [
    "## 基于Transformer的实现\n",
    "\n",
    "\n",
    "自注意力同时具有并行计算和最短的最大路径长度这两个优势，使用自注意力来设计深度架构是很有吸引力的。\n",
    "\n",
    "Transformer模型完全基于注意力机制，没有任何卷积层或循环神经网络层（`Vaswani.Shazeer.Parmar.ea.2017`），\n",
    "Transformer最初应用于在文本数据上Seq2Seq学习，现在广泛应用在语言、视觉、语音和强化学习领域。\n",
    "\n",
    "\n",
    "<img src=\"assets/transformer.svg\" width=\"40%\" height=\"40%\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9d4f18",
   "metadata": {},
   "source": [
    "上图展示了Transformer架构实现的Seq2Seq。\n",
    "Transformer 作为Encoder-Decoder架构的一个实例，整体由Encoder和Decoder组成的。\n",
    "\n",
    "* Transformer Encoder 由多个相同的层叠加而成。每层分别由*多头自注意力*（multi-head self-attention）层和*基于位置的前馈网络*（positionwise feed-forward network）两个子层（$\\mathrm{sublayer}$）构成。\n",
    "* Transformer Decoder 也是由多个相同的层叠加而成。每层分别由带有掩蔽的多头自注意力层、*编码器－解码器注意力*（encoder-decoder attention）层和基于位置的前馈网络构成。\n",
    "\n",
    "* Transformer中的残差连接和层规范化是训练非常深度模型的重要工具。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880c6e44",
   "metadata": {},
   "source": [
    "### 残差连接与层规范化（Add & Norm）\n",
    "\n",
    "受ResNet启发，子层间采用残差连接，并且使用广泛使用dropout和norm正则化技术。\n",
    "\n",
    "不同于CV常采用batchnorm规范化技术,NLP任务由于通常是变长序列，通常采用layernorm规范化技术。\n",
    "\n",
    "经过Add&Norm层，输入和输出形状不变。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9b16fd",
   "metadata": {},
   "source": [
    "### 基于位置的前馈网络（Positionwise FFN）\n",
    "\n",
    "实现上就是两层MLP。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7beef974",
   "metadata": {},
   "source": [
    "### Encoder Block\n",
    "\n",
    "Transformer Encoder任何子层不会改变输入形状。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48996bf",
   "metadata": {},
   "source": [
    "### Decoder Block\n",
    "\n",
    "* 在训练阶段，其输出序列的所有位置（时间步）的词元都是已知的\n",
    "* 在预测阶段，其输出序列的词元是逐个生成的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ec6524",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe652ef",
   "metadata": {},
   "source": [
    "\n",
    "* transformer是编码器－解码器架构的一个实践，尽管在实际情况中编码器或解码器可以单独使用。\n",
    "* 在transformer中，多头自注意力用于表示输入序列和输出序列，不过解码器必须通过掩蔽机制来保留自回归属性。\n",
    "* transformer模型中基于位置的前馈网络使用同一个多层感知机，作用是对所有序列位置的表示进行转换。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53e4aa5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-08T08:08:21.591925Z",
     "start_time": "2022-05-08T08:08:21.550715Z"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c58a3e",
   "metadata": {},
   "source": [
    "## references\n",
    "\n",
    "* [d2l](https://zh-v2.d2l.ai/chapter_attention-mechanisms/attention-scoring-functions.html)\n",
    "* [pytorch-seq2seq](https://github.com/bentrevett/pytorch-seq2seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d20cc80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
